{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need admin, other, and Location Type\n",
    "admin_df = pd.read_csv('../Data/2020 Domestic Violence - Administrative Data.csv', encoding='ISO-8859-1',skiprows=7)\n",
    "other_df = pd.read_csv('../Data/2020 Domestic Violence - Other Data.csv', encoding='ISO-8859-1',skiprows=7)\n",
    "# Location Type is a quarterly csv, compiling\n",
    "location_df = pd.DataFrame()\n",
    "for i in range(1,5):\n",
    "    if location_df.empty:\n",
    "        location_df = pd.read_csv('../Data/2020 Q1 Domestic Violence - Location Type.csv', encoding='ISO-8859-1',skiprows=6)\n",
    "    else:\n",
    "        L = pd.read_csv(f'../Data/2020 Q{i} Domestic Violence - Location Type.csv', encoding='ISO-8859-1',skiprows=6)\n",
    "        location_df = pd.concat([location_df, L], axis=0)\n",
    "        location_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69299, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are missing entries from one df to another...because date entry *facepalm with monitor\n",
    "# verifying dates are the same formate across df\n",
    "def change_date_Y_y(x):\n",
    "    try:\n",
    "        dt = datetime.strptime(x, \"%m/%d/%y\")\n",
    "    except ValueError:\n",
    "        dt = datetime.strptime(x, \"%m/%d/%Y\")\n",
    "    return dt.strftime(\"%m/%d/%y\")\n",
    "\n",
    "# specfically want to get rid of decimal places if they occur, and fix scientific notation \n",
    "# which is not recognized as the same from one table to another in long form. Long form is searchable in the og csvs, so\n",
    "# keep long form\n",
    "def change_incNum_long_noDec(x):\n",
    "    # will need to fix this first\n",
    "    # fix scientific notation to long form, ex: other ref '2.02007E+11' -> '20200700000'\n",
    "    if '+' in x:\n",
    "        x, n = x.split('E+')\n",
    "        # mult by power of 10, then convert back to str w/o '.0' tail\n",
    "        x = str(float(x)*10**int(n))[:-2]\n",
    "    # drop decimals if present, ex: admin ref '202001000000.00' -> '202001000000'\n",
    "    return x.split('.')[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Tables, Creating keys, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69299, 9)\n",
      "(69299, 9)\n",
      "(59976, 10)\n",
      "admin:  59976 | other:  69299 | location:  118124\n"
     ]
    }
   ],
   "source": [
    "# formatting incident date feature across tables\n",
    "admin_df['Incident Date'] = admin_df['Incident Date'].apply(change_date_Y_y)\n",
    "other_df['Incident Date'] = other_df['Incident Date'].apply(change_date_Y_y)\n",
    "location_df['Incident Date'] = location_df['Incident Date'].apply(change_date_Y_y)\n",
    "\n",
    "# formatting Incident Numbers feature across tables\n",
    "admin_df['Incident Number'] = admin_df['Incident Number'].apply(change_incNum_long_noDec)\n",
    "other_df['Incident Number'] = other_df['Incident Number'].apply(change_incNum_long_noDec)\n",
    "location_df['Incident Number'] = location_df['Incident Number'].apply(change_incNum_long_noDec)\n",
    "\n",
    "# creating key that will be identifiable across df\n",
    "admin_df['Incident_Key'] = admin_df['ORI'] + '<>' + admin_df['Incident Number'] + '<>' + admin_df['Incident Date']\n",
    "location_df['Incident_Key'] = location_df['ORI'] + '<>' + location_df['Incident Number'] + '<>' + location_df['Incident Date']\n",
    "other_df['Incident_Key'] = other_df['ORI'] + '<>' + other_df['Incident Number'] + '<>' + other_df['Incident Date']\n",
    "admin_df.drop_duplicates('Incident_Key', inplace=True)\n",
    "# Issue: format inconsistencies with ORIs keeps keys from working,\n",
    "# ex: 'TN0150100<>2.02001E+11<>01/02/20' vs 'TN0150100<>202001000000.00<>01/02/20'\n",
    "\n",
    "# location_df.drop_duplicates('Incident_Key', inplace=True)\n",
    "print('admin: ',len(admin_df), '| other: ',len(other_df), '| location: ',len(location_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 of 59976 records checked\n",
      "2000 of 59976 records checked\n",
      "3000 of 59976 records checked\n",
      "4000 of 59976 records checked\n",
      "5000 of 59976 records checked\n",
      "6000 of 59976 records checked\n",
      "7000 of 59976 records checked\n",
      "8000 of 59976 records checked\n",
      "9000 of 59976 records checked\n",
      "10000 of 59976 records checked\n",
      "11000 of 59976 records checked\n",
      "12000 of 59976 records checked\n",
      "13000 of 59976 records checked\n",
      "14000 of 59976 records checked\n",
      "15000 of 59976 records checked\n",
      "16000 of 59976 records checked\n",
      "17000 of 59976 records checked\n",
      "18000 of 59976 records checked\n",
      "19000 of 59976 records checked\n",
      "20000 of 59976 records checked\n",
      "21000 of 59976 records checked\n",
      "22000 of 59976 records checked\n",
      "23000 of 59976 records checked\n",
      "24000 of 59976 records checked\n",
      "25000 of 59976 records checked\n",
      "26000 of 59976 records checked\n",
      "27000 of 59976 records checked\n",
      "28000 of 59976 records checked\n",
      "29000 of 59976 records checked\n",
      "30000 of 59976 records checked\n",
      "31000 of 59976 records checked\n",
      "32000 of 59976 records checked\n",
      "33000 of 59976 records checked\n",
      "34000 of 59976 records checked\n",
      "35000 of 59976 records checked\n",
      "36000 of 59976 records checked\n",
      "37000 of 59976 records checked\n",
      "38000 of 59976 records checked\n",
      "39000 of 59976 records checked\n",
      "40000 of 59976 records checked\n",
      "41000 of 59976 records checked\n",
      "42000 of 59976 records checked\n",
      "43000 of 59976 records checked\n",
      "44000 of 59976 records checked\n",
      "45000 of 59976 records checked\n",
      "46000 of 59976 records checked\n",
      "47000 of 59976 records checked\n",
      "48000 of 59976 records checked\n",
      "49000 of 59976 records checked\n",
      "50000 of 59976 records checked\n",
      "51000 of 59976 records checked\n",
      "52000 of 59976 records checked\n",
      "53000 of 59976 records checked\n",
      "54000 of 59976 records checked\n",
      "55000 of 59976 records checked\n",
      "56000 of 59976 records checked\n",
      "57000 of 59976 records checked\n",
      "58000 of 59976 records checked\n",
      "59000 of 59976 records checked\n"
     ]
    }
   ],
   "source": [
    "# isn't the most efficient. Takes 5 min 39 sec to run. b\n",
    "\n",
    "mismatching = {\n",
    "    'other':[],\n",
    "    'location':[],\n",
    "    'both':[],\n",
    "}\n",
    "i, j = 0, len(admin_df)\n",
    "for key in admin_df.Incident_Key.unique():\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f'{i} of {j} records checked')\n",
    "    O = other_df.loc[other_df.Incident_Key == key]\n",
    "    L = location_df.loc[location_df.Incident_Key == key]\n",
    "    check = O.empty and L.empty\n",
    "    if not check:\n",
    "        continue\n",
    "    elif check:\n",
    "        mismatching['both'].append(key)\n",
    "    # elif O.empty: # turns out it's literally all or nothing for the mismatches\n",
    "    #     mismatching['other'].append(key)\n",
    "    # elif L.empty:\n",
    "    #     mismatching['location'].append(key)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN0410000<>201211000000<>12/11/20 TN0410000<>201211-1055<>12/11/20 TN0410000<>201210999999<>12/11/20\n"
     ]
    }
   ],
   "source": [
    "to_replace = {} # bad admin_df.Incident_Key : good Incident_Key\n",
    "\n",
    "for badkey in mismatching['both']:\n",
    "    # identify the correct key in location\n",
    "    ori, num, dt = badkey.split('<>')\n",
    "    L_key = location_df.loc[(location_df['Incident Date'] == dt) & (location_df.ORI == ori)].Incident_Key.values[0]\n",
    "    O_key = other_df.loc[(other_df['Incident Date'] == dt) & (other_df.ORI == ori)].Incident_Key.values[0]\n",
    "    \n",
    "    # verify they match\n",
    "    if L_key != O_key:\n",
    "        print(badkey, L_key, O_key)\n",
    "    else:\n",
    "        to_replace[badkey] = L_key\n",
    "    \n",
    "# after analyzing the anomoly there are 2 instances in each of the tables that use the L key and the O key, but L puts reverse order from O hence the mismatch\n",
    "# keeping with consistent error type \n",
    "to_replace['TN0410000<>201211000000<>12/11/20'] = 'TN0410000<>201210999999<>12/11/20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all Incident_Keys match across tables! \n",
    "admin_df.replace(to_replace=to_replace, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backing up these tables\n",
    "admin_df.to_csv('../Data/Mart2/admin.csv')\n",
    "other_df.to_csv('../Data/Mart2/other.csv')\n",
    "location_df.to_csv('../Data/Mart2/location_compiled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: \n",
    "# -City\n",
    "# -County\n",
    "# -Agency Name\n",
    "# -Pri Location\n",
    "# -Pop of City / County \n",
    "# -Rates per 1k, 10k, 100k residents\n",
    "# -Weigth (essentially how many offenses were reported in this incident)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
